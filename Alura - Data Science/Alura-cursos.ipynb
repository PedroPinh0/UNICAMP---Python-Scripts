{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Machine Learning: Introdução a classificação\n",
    "\n",
    "Não executar celular individualmente...necessário reiniciar todo o programa para evitar troca de valores entre variáveis com o mesmo nome em células diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bibliotecas\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from collections import Counter\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1 -1]\n",
      "Taxa de acerto é de 50.0 %\n"
     ]
    }
   ],
   "source": [
    "#Array de caracteristicas [gordo, pernas curtas, late]. 1 - Sim, 0 - Não\n",
    "porco1 = [1,1,0]\n",
    "porco2 = [1,0,1]\n",
    "porco3 = [0,1,0]\n",
    "\n",
    "cachorro1 = [1,1,1]\n",
    "cachorro2 = [0,1,1]\n",
    "cachorro3 = [0,1,1]\n",
    "\n",
    "dados = [porco1, porco2, porco3, cachorro1, cachorro2, cachorro3]\n",
    "marcações = [1,1,1,-1,-1,-1] # 1 - porco, -1 - cachorro\n",
    "\n",
    "#Dados para teste\n",
    "misterioso1 = [1,1,1]\n",
    "misterioso2 = [1,0,0]\n",
    "misterioso3 = [1,0,1]\n",
    "misterioso4 = [0,1,1]\n",
    "misteriosos = [misterioso1, misterioso2, misterioso3, misterioso4]\n",
    "\n",
    "#Treinamento do modelo\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(dados, marcações)\n",
    "\n",
    "#Prever para array 'misterioso'\n",
    "resultado = modelo.predict(misteriosos)\n",
    "print(resultado)\n",
    "\n",
    "#Estudo de margem de erro. Vamos agora comparar o resultado do nosso modelo para o array de testes com o real\n",
    "#resultado armagenado no array 'marcações_misterioso'\n",
    "\n",
    "#Resultado esperando para array 'misterioso':\n",
    "marcações_misterioso = [-1,1,-1,-1]\n",
    "\n",
    "#Se os resultados forem iguais o elemento no array será 0\n",
    "comp = marcações_misterioso - resultado \n",
    "acerto = [acertos for acertos in comp if acertos == 0]\n",
    "\n",
    "k, n = len(acerto), len(comp)\n",
    "print(\"Taxa de acerto é de\", k/n*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro caso real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa de acerto é 93.93939393939394 %\n"
     ]
    }
   ],
   "source": [
    "def carregar_acessos():\n",
    "    X = []\n",
    "    Y = []\n",
    "    arquivo = open('acesso_pagina.csv', 'r')\n",
    "    leitor = csv.reader(arquivo)\n",
    "    next(leitor)\n",
    "    \n",
    "    for acessou_home,acessou_como_funciona,acesso_contato,comprou in leitor:\n",
    "        X.append([int(acessou_home), int(acessou_como_funciona), int(acesso_contato)])\n",
    "        Y.append([int(comprou)])\n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "X, Y = carregar_acessos()\n",
    "Y = [item for sublist in Y for item in sublist]\n",
    "\n",
    "modeloAlura = MultinomialNB()\n",
    "modeloAlura.fit(X, Y)\n",
    "\n",
    "teste = [[1,0,1], [0,1,0], [1,0,0], [1,1,0], [1,1,1]]\n",
    "\n",
    "resultado = modeloAlura.predict(X)\n",
    "comparacao = resultado - Y\n",
    "\n",
    "acertos = [erro for erro in comparacao if erro == 0]\n",
    "\n",
    "taxa = len(acertos)/len(X)\n",
    "print(\"Taxa de acerto é\",taxa*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre utilizar os dados do treino nos testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa de acerto é 88.88888888888889 %\n"
     ]
    }
   ],
   "source": [
    "#No exemplo anterior testamos o modelo com os mesmos dados\n",
    "#que usamos para treinar...isso nos entrega uma taxa de\n",
    "#acerto falsa. Precisamos encontrar uma forma de saber,\n",
    "#de todos os nossos dados, quantos usar para treinar e \n",
    "#quantos usar para testar.\n",
    "\n",
    "def carregar_acessos():\n",
    "    X = []\n",
    "    Y = []\n",
    "    arquivo = open('acesso_pagina.csv', 'r')\n",
    "    leitor = csv.reader(arquivo)\n",
    "    next(leitor)\n",
    "    \n",
    "    for acessou_home,acessou_como_funciona,acesso_contato,comprou in leitor:\n",
    "        X.append([int(acessou_home), int(acessou_como_funciona), int(acesso_contato)])\n",
    "        Y.append([int(comprou)])\n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "X, Y = carregar_acessos()\n",
    "Y = [item for sublist in Y for item in sublist]\n",
    "\n",
    "treino_dados = X[:90]\n",
    "treino_marcacoes = Y[:90]\n",
    "\n",
    "teste_dados = X[-9:]\n",
    "teste_marcacoes = Y[-9:]\n",
    "\n",
    "modeloAlura = MultinomialNB()\n",
    "modeloAlura.fit(treino_dados, treino_marcacoes)\n",
    "\n",
    "resultado = modeloAlura.predict(teste_dados)\n",
    "comparacao = resultado - teste_marcacoes\n",
    "\n",
    "acertos = [erro for erro in comparacao if erro == 0]\n",
    "\n",
    "taxa = len(acertos)/len(teste_dados)\n",
    "print(\"Taxa de acerto é\",taxa*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando DataFrame e Caracteristicas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto é 82.0 %\n",
      "A taxa de acerto base é 82.0 %\n"
     ]
    }
   ],
   "source": [
    "#Vamos usar o pandas para ler os dados muito mais facil-\n",
    "#mente. O pandas consegue fazer todo o papel da função que\n",
    "#que haviamos criado sem se precisar com tipos de dados\n",
    "\n",
    "df = pd.read_csv('dados_cat.csv')\n",
    "X_df, Y_df = df[['home','busca','logado']], df['comprou']\n",
    "\n",
    "#Vamos transformar a coluna categorica 'algoritmo' em 3\n",
    "#colunas binárias\n",
    "X_df = pd.get_dummies(X_df)\n",
    " \n",
    "X = X_df.values\n",
    "Y = Y_df.values\n",
    "\n",
    "n = int(0.9 * len(X))\n",
    "m = int(len(X) - n)\n",
    "\n",
    "treino_dados     = X[:n] \n",
    "treino_marcacoes = Y[:n]\n",
    "teste_dados      = X[-m:]\n",
    "teste_marcacoes  = Y[-m:]\n",
    "\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(treino_dados,treino_marcacoes)\n",
    "\n",
    "resultados = modelo.predict(teste_dados)\n",
    "comparacao = resultados - teste_marcacoes\n",
    "\n",
    "acertos = [acerto for acerto in comparacao if acerto == 0]\n",
    "\n",
    "print(\"A taxa de acerto é\",len(acertos)/len(comparacao)*100,\"%\")\n",
    "\n",
    "#Mas essa taxa de acerto é um valor bom? Como saber? Vamos fazer um algoritmo burro que chuta o mesmo valor para\n",
    "#todos os testes\n",
    "\n",
    "acerto_burro = max(len(teste_marcacoes[teste_marcacoes == 1]), len(teste_marcacoes[teste_marcacoes == 0]))\n",
    "taxa_acerto = 100 * acerto_burro/len(teste_marcacoes)\n",
    "print(\"A taxa de acerto base é\", taxa_acerto, \"%\")\n",
    "\n",
    "#Ou seja, o nosso algoritmo é tão ruim quanto um que chuta o valor que mais aparece nos dados. Precisamos melhorar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apresentando AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto AdaBoostClassifier: 75.0\n",
      "A taxa de acerto MultinomialNB: 75.0\n",
      "A taxa de acerto base é 62.5 %\n"
     ]
    }
   ],
   "source": [
    "#Vamos testar o modelo usando AdaBoostClassifier e o MultinomialNB para encontrar a melhor \n",
    "#versão do nosso modelo com varias combinações de variaveis utilizadas\n",
    "\n",
    "def fit_and_predict(nome, modelo, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes):\n",
    "    \n",
    "    modelo.fit(treino_dados,treino_marcacoes)\n",
    "    resultados = modelo.predict(teste_dados)\n",
    "    comparacao = resultados - teste_marcacoes\n",
    "    acertos = [acerto for acerto in comparacao if acerto == 0]\n",
    "    taxa_de_acertos = 100*len(acertos)/len(comparacao)\n",
    "    msg = \"A taxa de acerto {0}: {1}\".format(nome, taxa_de_acertos)\n",
    "    print(msg)\n",
    "    \n",
    "df = pd.read_csv('dados_cat2.csv')\n",
    "X_df, Y_df = df[['home','busca','logado']], df['comprou']\n",
    "\n",
    "X_df = pd.get_dummies(X_df)\n",
    " \n",
    "X = X_df.values\n",
    "Y = Y_df.values\n",
    "\n",
    "n = int(0.9 * len(X))\n",
    "m = int(len(X) - n)\n",
    "\n",
    "treino_dados     = X[:n] \n",
    "treino_marcacoes = Y[:n]\n",
    "teste_dados      = X[-m:]\n",
    "teste_marcacoes  = Y[-m:]\n",
    "\n",
    "modelo = AdaBoostClassifier()\n",
    "fit_and_predict(\"AdaBoostClassifier\",modelo, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "modelo = MultinomialNB()\n",
    "fit_and_predict(\"MultinomialNB\", modelo, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "acerto_burro = max(len(teste_marcacoes[teste_marcacoes == 1]), len(teste_marcacoes[teste_marcacoes == 0]))\n",
    "taxa_acerto = 100 * acerto_burro/len(teste_marcacoes)\n",
    "print(\"A taxa de acerto base é\", taxa_acerto, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando treino, teste e validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto AdaBoostClassifier: 85.71428571428571\n",
      "A taxa de acerto MultinomialNB: 85.71428571428571\n",
      "A taxa de acerto base é 85.71428571428571 %\n",
      "A taxa de acerto do vencedor no mundo real: 85.71428571428571\n"
     ]
    }
   ],
   "source": [
    "def fit_and_predict(nome, modelo, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes):\n",
    "    \n",
    "    modelo.fit(treino_dados,treino_marcacoes)\n",
    "    resultados = modelo.predict(teste_dados)\n",
    "    comparacao = resultados - teste_marcacoes\n",
    "    acertos = [acerto for acerto in comparacao if acerto == 0]\n",
    "    taxa_de_acertos = 100*len(acertos)/len(comparacao)\n",
    "    msg = \"A taxa de acerto {0}: {1}\".format(nome, taxa_de_acertos)\n",
    "    print(msg)\n",
    "    return taxa_de_acertos\n",
    "    \n",
    "df = pd.read_csv('dados_cat2.csv')\n",
    "X_df, Y_df = df[['home','busca','logado']], df['comprou']\n",
    "\n",
    "X_df = pd.get_dummies(X_df)\n",
    " \n",
    "X = X_df.values\n",
    "Y = Y_df.values\n",
    "\n",
    "tamanho_treino = int(0.8 * len(Y))\n",
    "tamanho_teste = int(0.1 * len(Y))\n",
    "tamanho_validacao = int(tamanho_treino - tamanho_teste)\n",
    "\n",
    "treino_dados        = X[:tamanho_treino] \n",
    "treino_marcacoes    = Y[:tamanho_treino]\n",
    "teste_dados         = X[tamanho_treino:tamanho_treino + tamanho_teste]\n",
    "teste_marcacoes     = Y[tamanho_treino:tamanho_treino + tamanho_teste]\n",
    "validacao_dados     = X[-tamanho_teste:] \n",
    "validacao_marcacoes = Y[-tamanho_teste:]\n",
    "\n",
    "modeloAda = AdaBoostClassifier()\n",
    "resultado_adaboost = fit_and_predict(\"AdaBoostClassifier\", modeloAda, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "modeloNB = MultinomialNB()\n",
    "resultado_multinomial = fit_and_predict(\"MultinomialNB\", modeloNB, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "acerto_burro = max(len(teste_marcacoes[teste_marcacoes == 1]), len(teste_marcacoes[teste_marcacoes == 0]))\n",
    "taxa_acerto = 100 * acerto_burro/len(teste_marcacoes)\n",
    "print(\"A taxa de acerto base é\", taxa_acerto, \"%\")\n",
    "\n",
    "if resultado_adaboost > resultado_multinomial:\n",
    "    vencedor = modeloAda\n",
    "else: vencedor = modeloNB\n",
    "    \n",
    "resultado_real = vencedor.predict(validacao_dados)\n",
    "\n",
    "comparacao_real = resultado_real - validacao_marcacoes\n",
    "acertos_real = [acerto for acerto in comparacao_real if acerto == 0]\n",
    "taxa_de_acertos_real = 100*len(acertos_real)/len(comparacao_real)\n",
    "msg = \"A taxa de acerto do vencedor {0}: {1}\".format(\"no mundo real\", taxa_de_acertos_real)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Machine Learning II: Avançando com tipos diferentes de classificação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mais algoritimos de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto OneVsOne: 100.0\n",
      "A taxa de acerto OneVsRest: 90.9090909090909\n",
      "A taxa de acerto AdaBoostClassifier: 68.18181818181819\n",
      "A taxa de acerto MultinomialNB: 72.72727272727273\n",
      "Taxa de acerto base: 81.81818181818181 %\n",
      "A taxa de acerto do vencedor no mundo real: 100.0\n"
     ]
    }
   ],
   "source": [
    "def fit_and_predict(nome, modelo, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes):\n",
    "    \n",
    "    modelo.fit(treino_dados,treino_marcacoes)\n",
    "    resultados = modelo.predict(teste_dados)\n",
    "    comparacao = resultados - teste_marcacoes\n",
    "    acertos = [acerto for acerto in comparacao if acerto == 0]\n",
    "    taxa_de_acertos = 100*len(acertos)/len(comparacao)\n",
    "    msg = \"A taxa de acerto {0}: {1}\".format(nome, taxa_de_acertos)\n",
    "    print(msg)\n",
    "    return taxa_de_acertos\n",
    "    \n",
    "df = pd.read_csv('situacao_do_cliente.csv')\n",
    "X_df, Y_df = df[['recencia','frequencia','semanas_de_inscricao']], df['situacao']\n",
    "\n",
    "X_df = pd.get_dummies(X_df)\n",
    " \n",
    "X = X_df.values\n",
    "Y = Y_df.values\n",
    "\n",
    "tamanho_treino = int(0.8 * len(Y))\n",
    "tamanho_teste = int(0.1 * len(Y))\n",
    "tamanho_validacao = int(tamanho_treino - tamanho_teste)\n",
    "\n",
    "treino_dados        = X[:tamanho_treino] \n",
    "treino_marcacoes    = Y[:tamanho_treino]\n",
    "teste_dados         = X[tamanho_treino:tamanho_treino + tamanho_teste]\n",
    "teste_marcacoes     = Y[tamanho_treino:tamanho_treino + tamanho_teste]\n",
    "validacao_dados     = X[-tamanho_teste:] \n",
    "validacao_marcacoes = Y[-tamanho_teste:]\n",
    "\n",
    "modeloOVO = OneVsOneClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVO = fit_and_predict(\"OneVsOne\", modeloOVO, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "modeloOVR = OneVsRestClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVR = fit_and_predict(\"OneVsRest\", modeloOVR, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "modeloAda = AdaBoostClassifier()\n",
    "resultado_adaboost = fit_and_predict(\"AdaBoostClassifier\", modeloAda, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "modeloNB = MultinomialNB()\n",
    "resultado_multinomial = fit_and_predict(\"MultinomialNB\", modeloNB, treino_dados, treino_marcacoes, teste_dados, teste_marcacoes)\n",
    "\n",
    "acerto_base = max(Counter(validacao_marcacoes).values())\n",
    "taxa_de_acerto_base = 100 * acerto_base / len(validacao_marcacoes)\n",
    "print(\"Taxa de acerto base:\", taxa_de_acerto_base, \"%\")\n",
    "\n",
    "resultados = {}\n",
    "resultados[resultado_OVO] = modeloOVO\n",
    "resultados[resultado_OVR] = modeloOVR\n",
    "resultados[resultado_adaboost] = modeloAda\n",
    "resultados[resultado_multinomial] = modeloNB\n",
    "\n",
    "vencedor = resultados[max(resultados)]\n",
    "\n",
    "resultado_real = vencedor.predict(validacao_dados)\n",
    "\n",
    "comparacao_real = resultado_real - validacao_marcacoes\n",
    "acertos_real = [acerto for acerto in comparacao_real if acerto == 0]\n",
    "taxa_de_acertos_real = 100*len(acertos_real)/len(comparacao_real)\n",
    "msg = \"A taxa de acerto do vencedor {0}: {1}\".format(\"no mundo real\", taxa_de_acertos_real)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto OneVsOne: 99.44444444444444\n",
      "A taxa de acerto OneVsRest: 92.31381148950808\n",
      "A taxa de acerto AdaBoostClassifier: 76.29471964224285\n",
      "A taxa de acerto MultinomialNB: 82.99772101823186\n",
      "Taxa de acerto base: 75.55555555555556 %\n",
      "A taxa de acerto do vencedor no mundo real: 100.0\n"
     ]
    }
   ],
   "source": [
    "def score(nome, modelo, treino_dados, treino_marcacoes):\n",
    "    k = 10\n",
    "    scores = cross_val_score(modelo, treino_dados, treino_marcacoes, cv = k)\n",
    "    taxa_de_acertos = 100 * np.mean(scores)\n",
    "    msg = \"A taxa de acerto {0}: {1}\".format(nome, taxa_de_acertos)\n",
    "    print(msg)\n",
    "    return taxa_de_acertos\n",
    "    \n",
    "df = pd.read_csv('situacao_do_cliente.csv')\n",
    "X_df, Y_df = df[['recencia','frequencia','semanas_de_inscricao']], df['situacao']\n",
    "\n",
    "X_df = pd.get_dummies(X_df)\n",
    " \n",
    "X = X_df.values\n",
    "Y = Y_df.values\n",
    "\n",
    "porcentagem = 0.8\n",
    "tamanho_treino = int(porcentagem * len(Y))\n",
    "\n",
    "treino_dados        = X[0:tamanho_treino] \n",
    "treino_marcacoes    = Y[0:tamanho_treino]\n",
    "\n",
    "validacao_dados     = X[tamanho_treino:] \n",
    "validacao_marcacoes = Y[tamanho_treino:]\n",
    "\n",
    "modeloOVO = OneVsOneClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVO = score(\"OneVsOne\", modeloOVO, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloOVR = OneVsRestClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVR = score(\"OneVsRest\", modeloOVR, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloAda = AdaBoostClassifier()\n",
    "resultado_adaboost = score(\"AdaBoostClassifier\", modeloAda, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloNB = MultinomialNB()\n",
    "resultado_multinomial = score(\"MultinomialNB\", modeloNB, treino_dados, treino_marcacoes)\n",
    "\n",
    "acerto_base = max(Counter(validacao_marcacoes).values())\n",
    "taxa_de_acerto_base = 100 * acerto_base / len(validacao_marcacoes)\n",
    "print(\"Taxa de acerto base:\", taxa_de_acerto_base, \"%\")\n",
    "\n",
    "resultados = {}\n",
    "resultados[resultado_OVO] = modeloOVO\n",
    "resultados[resultado_OVR] = modeloOVR\n",
    "resultados[resultado_adaboost] = modeloAda\n",
    "resultados[resultado_multinomial]= modeloNB\n",
    "\n",
    "vencedor = resultados[max(resultados)]\n",
    "vencedor.fit(treino_dados, treino_marcacoes)\n",
    "\n",
    "resultado_real = vencedor.predict(validacao_dados)\n",
    "comparacao_real = resultado_real - validacao_marcacoes\n",
    "acertos_real = [acerto for acerto in comparacao_real if acerto == 0]\n",
    "taxa_de_acertos_real = 100*len(acertos_real)/len(comparacao_real)\n",
    "msg = \"A taxa de acerto do vencedor {0}: {1}\".format(\"no mundo real\", taxa_de_acertos_real)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando um dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto OneVsOne: 65.66666666666666\n",
      "A taxa de acerto OneVsRest: 72.33333333333334\n",
      "A taxa de acerto AdaBoostClassifier: 47.33333333333333\n",
      "A taxa de acerto MultinomialNB: 71.50000000000001\n",
      "Taxa de acerto base: 44.44444444444444 %\n",
      "A taxa de acerto do vencedor no mundo real: 88.88888888888889\n"
     ]
    }
   ],
   "source": [
    "classificacoes = pd.read_csv(\"emails.csv\")\n",
    "\n",
    "textos_puros = classificacoes['email']\n",
    "textosQuebrados = textos_puros.str.lower().str.split(' ')\n",
    "\n",
    "dicionario = set()\n",
    "for lista in textosQuebrados:\n",
    "    dicionario.update(lista)\n",
    "\n",
    "tuplas = zip(dicionario, range(total_de_palavras))\n",
    "tradutor = {palavra: indice for palavra,indice in tuplas}\n",
    "\n",
    "def vetorizar_texto(texto, tradutor):\n",
    "    vetor = [0] * len(tradutor)\n",
    "    for palavra in texto:\n",
    "        if palavra in tradutor:\n",
    "            vetor[tradutor[palavra]] += 1\n",
    "    return vetor\n",
    "        \n",
    "def score(nome, modelo, treino_dados, treino_marcacoes):\n",
    "    k = 10\n",
    "    scores = cross_val_score(modelo, treino_dados, treino_marcacoes, cv = k)\n",
    "    taxa_de_acertos = 100 * np.mean(scores)\n",
    "    msg = \"A taxa de acerto {0}: {1}\".format(nome, taxa_de_acertos)\n",
    "    print(msg)\n",
    "    return taxa_de_acertos\n",
    "\n",
    "vetores_de_texto = [vetorizar_texto(texto, tradutor) for texto in textosQuebrados]\n",
    "\n",
    "X, Y = np.array(vetores_de_texto), np.array(classificacoes['classificacao'].tolist())\n",
    "\n",
    "porcentagem_de_treino = 0.8\n",
    "\n",
    "tamanho_treino = int(porcentagem_de_treino * len(Y))\n",
    "tamanho_validacao = len(Y) - tamanho_treino\n",
    "\n",
    "treino_dados        = X[0:tamanho_treino] \n",
    "treino_marcacoes    = Y[0:tamanho_treino]\n",
    "\n",
    "validacao_dados     = X[tamanho_treino:] \n",
    "validacao_marcacoes = Y[tamanho_treino:]\n",
    "\n",
    "modeloOVO = OneVsOneClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVO = score(\"OneVsOne\", modeloOVO, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloOVR = OneVsRestClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVR = score(\"OneVsRest\", modeloOVR, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloAda = AdaBoostClassifier()\n",
    "resultado_adaboost = score(\"AdaBoostClassifier\", modeloAda, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloNB = MultinomialNB()\n",
    "resultado_multinomial = score(\"MultinomialNB\", modeloNB, treino_dados, treino_marcacoes)\n",
    "\n",
    "acerto_base = max(Counter(validacao_marcacoes).values())\n",
    "taxa_de_acerto_base = 100 * acerto_base / len(validacao_marcacoes)\n",
    "print(\"Taxa de acerto base:\", taxa_de_acerto_base, \"%\")\n",
    "\n",
    "resultados = {}\n",
    "resultados[resultado_OVO] = modeloOVO\n",
    "resultados[resultado_OVR] = modeloOVR\n",
    "resultados[resultado_adaboost] = modeloAda\n",
    "resultados[resultado_multinomial]= modeloNB\n",
    "\n",
    "vencedor = resultados[max(resultados)]\n",
    "vencedor.fit(treino_dados, treino_marcacoes)\n",
    "\n",
    "resultado_real = vencedor.predict(validacao_dados)\n",
    "comparacao_real = resultado_real - validacao_marcacoes\n",
    "acertos_real = [acerto for acerto in comparacao_real if acerto == 0]\n",
    "taxa_de_acertos_real = 100*len(acertos_real)/len(comparacao_real)\n",
    "msg = \"A taxa de acerto do vencedor {0}: {1}\".format(\"no mundo real\", taxa_de_acertos_real)\n",
    "print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpando dados para melhorar eficiência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de acerto OneVsOne: 80.16666666666666\n",
      "A taxa de acerto OneVsRest: 80.16666666666666\n",
      "A taxa de acerto AdaBoost: 59.33333333333333\n",
      "A taxa de acerto MultinomialNB: 83.0\n",
      "Taxa de acerto base: 44.44444444444444 %\n",
      "A taxa de acerto do vencedor no mundo real: 77.77777777777777\n"
     ]
    }
   ],
   "source": [
    "#PASSOS:\n",
    "# Transformar em mínusculo \n",
    "# Separar palavras e pontuações\n",
    "# Remover stopwords\n",
    "# Transformar a palavra somente na sua raiz (sem genero, plural...)\n",
    "# Remover com menos de 3 caract. (Para remover pontuação)\n",
    "\n",
    "classificacoes = pd.read_csv(\"emails.csv\")\n",
    "\n",
    "textos_puros = classificacoes['email']\n",
    "\n",
    "frase_min = textos_puros.str.lower()\n",
    "textosQuebrados = [nltk.tokenize.word_tokenize(frase) for frase in frase_min]\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "dicionario = set()\n",
    "\n",
    "for lista in textosQuebrados:\n",
    "    validas = [stemmer.stem(palavra) for palavra in lista if palavra not in stopwords and len(palavra) > 2]\n",
    "    dicionario.update(validas)\n",
    "\n",
    "tuplas = zip(dicionario, range(total_de_palavras))\n",
    "tradutor = {palavra: indice for palavra,indice in tuplas}\n",
    "\n",
    "def vetorizar_texto(texto, tradutor):\n",
    "    vetor = [0] * len(tradutor)\n",
    "    for palavra in texto:\n",
    "        if len(palavra) > 0 and stemmer.stem(palavra) in tradutor:\n",
    "            vetor[tradutor[stemmer.stem(palavra)]] += 1\n",
    "    return vetor\n",
    "        \n",
    "def score(nome, modelo, treino_dados, treino_marcacoes):\n",
    "    k = 10\n",
    "    scores = cross_val_score(modelo, treino_dados, treino_marcacoes, cv = k)\n",
    "    taxa_de_acertos = 100 * np.mean(scores)\n",
    "    msg = \"A taxa de acerto {0}: {1}\".format(nome, taxa_de_acertos)\n",
    "    print(msg)\n",
    "    return taxa_de_acertos\n",
    "\n",
    "vetores_de_texto = [vetorizar_texto(texto, tradutor) for texto in textosQuebrados]\n",
    "\n",
    "X, Y = np.array(vetores_de_texto), np.array(classificacoes['classificacao'].tolist())\n",
    "\n",
    "porcentagem_de_treino = 0.8\n",
    "\n",
    "tamanho_treino = int(porcentagem_de_treino * len(Y))\n",
    "tamanho_validacao = len(Y) - tamanho_treino\n",
    "\n",
    "treino_dados        = X[0:tamanho_treino] \n",
    "treino_marcacoes    = Y[0:tamanho_treino]\n",
    "\n",
    "validacao_dados     = X[tamanho_treino:] \n",
    "validacao_marcacoes = Y[tamanho_treino:]\n",
    "\n",
    "modeloOVO = OneVsOneClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVO = score(\"OneVsOne\", modeloOVO, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloOVR = OneVsRestClassifier(LinearSVC(random_state = 0))\n",
    "resultado_OVR = score(\"OneVsRest\", modeloOVR, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloAda = AdaBoostClassifier()\n",
    "resultado_adaboost = score(\"AdaBoost\", modeloAda, treino_dados, treino_marcacoes)\n",
    "\n",
    "modeloNB = MultinomialNB()\n",
    "resultado_multinomial = score(\"MultinomialNB\", modeloNB, treino_dados, treino_marcacoes)\n",
    "\n",
    "acerto_base = max(Counter(validacao_marcacoes).values())\n",
    "taxa_de_acerto_base = 100 * acerto_base / len(validacao_marcacoes)\n",
    "print(\"Taxa de acerto base:\", taxa_de_acerto_base, \"%\")\n",
    "\n",
    "resultados = {}\n",
    "resultados[resultado_OVO] = modeloOVO\n",
    "resultados[resultado_OVR] = modeloOVR\n",
    "resultados[resultado_adaboost] = modeloAda\n",
    "resultados[resultado_multinomial]= modeloNB\n",
    "\n",
    "vencedor = resultados[max(resultados)]\n",
    "vencedor.fit(treino_dados, treino_marcacoes)\n",
    "\n",
    "resultado_real = vencedor.predict(validacao_dados)\n",
    "comparacao_real = resultado_real - validacao_marcacoes\n",
    "acertos_real = [acerto for acerto in comparacao_real if acerto == 0]\n",
    "taxa_de_acertos_real = 100*len(acertos_real)/len(comparacao_real)\n",
    "msg = \"A taxa de acerto do vencedor {0}: {1}\".format(\"no mundo real\", taxa_de_acertos_real)\n",
    "print(msg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
